{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrVPzm6KzWma"
   },
   "source": [
    "# Working with Zooniverse data\n",
    "\n",
    "For this workshop we will be working with the Zooniverse's data aggregation code `panoptes-aggregation`.  This code is designed to work directly with the data dumps exported from projects `Data Export` page.\n",
    "\n",
    "General documentation for the aggregation code can be found on https://aggregation-caesar.zooniverse.org/docs\n",
    "\n",
    "## Installing the code\n",
    "\n",
    "### Local computer\n",
    "\n",
    "Install python 3, the easiest way to do this is to download the Anaconda build from: https://www.anaconda.com/download/.  This will pre-install many of the packages needed for the aggregation code.\n",
    "\n",
    "Open a terminal and run: `pip install panoptes-aggregation[gui]`\n",
    "\n",
    "Download the folder containing the [example data](https://drive.google.com/drive/folders/1rLPs3jFsop9dw7Gst6AF0bjhFw6EZ1jc?usp=sharing)\n",
    "\n",
    "### Google colab\n",
    "\n",
    "Google colab is a service that runs python code in the cloud\n",
    "\n",
    "1. Sign into google drive\n",
    "2. Make a copy of the [example data](https://drive.google.com/drive/folders/1rLPs3jFsop9dw7Gst6AF0bjhFw6EZ1jc?usp=sharing) in your own google drive (easiest way is to download and re-uplaod the folder)\n",
    "3. Right click the `Working_with_data.ipynb` file > Open with > Google Colaboratory\n",
    " 1. if this is not an option click \"Connect more apps\", search for \"Google Colaboratory\", enable it, and refresh the page.\n",
    "4. Run the cell below to install the needed packages (it will take a few mins to finish)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6BVpClIzSCl"
   },
   "outputs": [],
   "source": [
    "!pip install panoptes_aggregation --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmaEMsgO0Klw"
   },
   "source": [
    "5. Run the cell below to mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amNr3p6G0E4l"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SAl_jSJ078S"
   },
   "source": [
    "6. Run the cell below to change director to the example folder (adjust if you have renamed the example folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 713,
     "status": "ok",
     "timestamp": 1606147054963,
     "user": {
      "displayName": "Coleman Krawczyk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggyy0vPodL74vKupL7ewNWOXnrC-ItpID-aDM5v=s64",
      "userId": "17600869181314191626"
     },
     "user_tz": 0
    },
    "id": "7K_K1NS10rra"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/agg_workshop_2020/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYa1sVVzyA8m"
   },
   "source": [
    "## Zooniverse data exports\n",
    "\n",
    "Zooniverse projects provide a large amount of data to research teams.  These data can be exported from the `Data Export` tab on a project's `Lab` page.\n",
    "\n",
    "### Classification export\n",
    "\n",
    "This `csv` file has one row for every classification submitted for a project.  This files has the following columns:\n",
    "\n",
    "* `classification_id`: A unique ID number assigned to each classification\n",
    "* `user_name`: The name of the user that submitted the classification.  Non logged-in users are assigned a unique name based on (a hashed version of) their IP address.\n",
    "* `user_id`: User ID number is provided for logged-in users\n",
    "* `user_ip`: A hashed version of the user's IP address (original IP addresses are not provided for privacy reasons)\n",
    "* `workflow_id`: The ID number for the workflow the classification was made on\n",
    "* `workflow_name`: The name of the workflow\n",
    "* `workflow_version`: The major and minor workflow version for the classification\n",
    "* `created_at`: The `UTC` timestamp for the classification\n",
    "* `gold_standard`: Identifies if the classification was made on a gold standard subject\n",
    "* `expert`: Identifies if the classification was made in \"expert\" mode\n",
    "* `metadata`: A `JSON` blob containing additional metadata about the classification (e.g. browser size, browser user agent, classification duration, etc...)\n",
    "* `annotations`: A `JSON` blob with the annotations made for each task in the workflow.  The exact shape of this blob is dependent on the shape of the workflow.\n",
    "* `subject_data`: A `JSON` blob with the metadata associated with the subject that was classified.  The exact shape of this blob is dependent on the metadata uploaded to each subject\n",
    "* `subject_ids`: The ID number for the subject classified\n",
    "\n",
    "### Subject export\n",
    "\n",
    "This `csv` file has one row for every subject uploaded to a project.  This file has the following columns:\n",
    "\n",
    "* `subject_id`: A unique ID number assigned to each subject as they are uploaded\n",
    "* `project_id`: The ID number for the project\n",
    "* `workflow_id`: The workflow ID the subject is associated with\n",
    "* `subject_set_id`: The ID of the subject set the subject is connected to\n",
    "* `metadata`: A `JSON` blob with the subject's metadata\n",
    "* `locations`: A `JSON` blob with the URL to each `frame` of the subject\n",
    "* `classifications_count`: How many volunteers have classified the subject\n",
    "* `retired_at`: If the subject is retired this is the `UTC` timestamp for when it was retired\n",
    "* `retirement_reason`: The reason why it was retired\n",
    "\n",
    "### Workflows export\n",
    "\n",
    "This `csv` file has the information for every major version of a workflow.  This file has the following columns:\n",
    "\n",
    "* `workflow_id`: The ID number for the workflow\n",
    "* `display_name`: The display name for the workflow\n",
    "* `version`: The major version number (changes when a task is edited on the workflow)\n",
    "* `active`: `true` if the workflow is active\n",
    "* `classifications_count`: How many classifications have been made on the workflow\n",
    "* `primary_language`: The language code for the workflow\n",
    "* `first_task`: The task key for the first task\n",
    "* `retired_set_member_subjects_count`: The number of retired subjects from the workflow\n",
    "* `tasks`: A `JSON` blob showing the full workflow structure\n",
    "* `retirement`: The retirement rules for the workflow\n",
    "* `strings`: A `JSON` blob containing all the text associated with the workflow\n",
    "* `minor_version`: The minor workflow version number (changes when text is edited on the workflow)\n",
    "\n",
    "All other columns are not typically used and are for experimental or more advanced workflow setups.\n",
    "\n",
    "## How the aggregation code works\n",
    "\n",
    "Aggregation is done in a three step process.\n",
    "\n",
    "1. Configure `extractors` and `reducers`\n",
    "    1. Check over configuration files and edit them as needed\n",
    "2. `Extract` the data from each classification into a more useful data format (i.e. flatten and normalize the data)\n",
    "3. `Reduce` the the extracts for each subject together (i.e. aggregate the data)\n",
    "\n",
    "## Processing the example Penguin Watch data\n",
    "\n",
    "If you are comfortable using the command line, open a terminal and navigate the folder containing the example data.  If you are working with the anaconda build on Windows you can using the anaconda launcher to boot up a `Jupyter lab` tab in your web browser.  Use the left panel to navigate to the folder with the example data then launch a terminal.\n",
    "\n",
    "A selection of each of the above export files for the Penguin Watch project has been provided for this workshop.  You should make a new folder called `aggregation` to direct the output of these scripts.  These steps outline how to use the command line to run all of the aggregation scripts, but a GUI is available by running `panoptes_aggregation_gui` from the terminal.\n",
    "\n",
    "### Run configuration script\n",
    "\n",
    "The auto-config script will detect the shape of a project's workflow and select the default extractor and reducers to use.  For this example we want to configuration files for workflow `6465` version `52.76`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E6mLcIouyA8n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Extractor config to:\n",
      "/Volumes/Work/agg_workshop_2020/aggregation_results/Extractor_config_workflow_6465_V52.76.yaml\n",
      "\n",
      "Saving Reducer config to:\n",
      "/Volumes/Work/agg_workshop_2020/aggregation_results/Reducer_config_workflow_6465_V52.76_point_extractor_by_frame.yaml\n",
      "\n",
      "Saving Reducer config to:\n",
      "/Volumes/Work/agg_workshop_2020/aggregation_results/Reducer_config_workflow_6465_V52.76_question_extractor.yaml\n",
      "\n",
      "Saving Reducer config to:\n",
      "/Volumes/Work/agg_workshop_2020/aggregation_results/Reducer_config_workflow_6465_V52.76_shortcut_extractor.yaml\n",
      "\n",
      "Saving task key look up table to:\n",
      "/Volumes/Work/agg_workshop_2020/aggregation_results/Task_labels_workflow_6465_V52.76.yaml\n"
     ]
    }
   ],
   "source": [
    "!panoptes_aggregation config Penguin-Watch-Example-data-dumps/penguin-watch-workflows.csv 6465 -v 52 -m 76 -d aggregation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_PSnDnbyA8p"
   },
   "source": [
    "See `panoptes_aggregation config -h` for help text explaining each of these inputs.\n",
    "\n",
    "This will crate four new files:\n",
    "\n",
    "* `Extractor_config_workflow_6465_V52.yaml`: The configuration file for the extractors\n",
    "* `Reducer_config_workflow_6465_V52.76_shortcut_extractor.yaml`: The configuration file for the shourtcut reducer\n",
    "* `Reducer_config_workflow_6465_V52_question_extractor.yaml`: The configuration file for the question reducer\n",
    "* `Reducer_config_workflow_6465_V52_point_extractor_by_frame.yaml`: The configuration file for the point reducer\n",
    "* `Task_labels_workflow_6465_V52.76.yaml`: A file with a look up table that matches the workflow task keys with the text associated with them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgQe6CeuyA8p",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Edit the extractor config file\n",
    "\n",
    "Task `T4` was never used in the final project so it can be removed from the config file. Today we are not intrested in task `T0` tool `3` (\"other\") so we will remove it and it's subtask form the config file. The final file should look like:\n",
    "\n",
    "```yaml\n",
    "extractor_config:\n",
    "    point_extractor_by_frame:\n",
    "    -   task: T0\n",
    "        tools:\n",
    "        - 0\n",
    "        - 1\n",
    "        - 2\n",
    "    question_extractor:\n",
    "    -   task: T1\n",
    "    shortcut_extractor:\n",
    "    -   task: T6\n",
    "workflow_id: 6465\n",
    "workflow_version: '52.76'\n",
    "\n",
    "```\n",
    "\n",
    "#### Converting this extractor config for use on Caesar\n",
    "\n",
    "All of the extractors available in within `panoptes_aggregation` can be used as \"external extractors\" on Caesar.\n",
    "\n",
    "1. On the \"Extractors\" tab in Caesar click \"Create Extractor\" and select \"External\"\n",
    "2. Enter a unique \"key\" to reference this extractor later\n",
    "    - This value will be used in the reducer later and will show up in the data export\n",
    "3. Enter the \"URL\" as `https://aggregation-caesar.zooniverse.org/extractors/<extractor name>?task=<task ID>`\n",
    "    - The `<task ID>` value is found next to the `task:` key for each extractor in the config file\n",
    "    - For our example this would be \n",
    "    ```\n",
    "    https://aggregation-caesar.zooniverse.org/extractors/point_extractor_by_frame?task=T0\n",
    "    ```\n",
    "4. Enter the \"Minimum workflow version\"\n",
    "    - All version above this number will be passed through this extractor\n",
    "    - `52.76` for the example above (the value next to `workflow_version:`)\n",
    "5. Click \"Create External extractor\"\n",
    "\n",
    "Note: For question tasks you will typically want to use the built in question extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the extractors\n",
    "\n",
    "The extraction script will create one `csv` file for each type of extractor being used.  In this case there will be two files created, one for `point_extractor_by_frame` and one for `question_extractor`.\n",
    "\n",
    "See `panoptes_aggregation extract -h` for help text explaining each of these inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rHXYi2NDyA8p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: 100% |#############################################| Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "!panoptes_aggregation extract Penguin-Watch-Example-data-dumps/penguin-watch-classifications-trim.csv aggregation_results/Extractor_config_workflow_6465_V52.76.yaml -o example -d aggregation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a-NpnFhyA8q"
   },
   "source": [
    "### Edit the reducer config files\n",
    "\n",
    "There are no configuration parameters for the question reducer so that files does not need to be edited, but for the point reducer we should switch it from the default `DBSCAN` reducer to the `HDBSCAN` one.  We are making this switch since the Penguin Watch subjects have a large depth-of-field that causes the point clusters to be different densities across the image.\n",
    "\n",
    "We can also use this config file to set the `min_cluster_size` and `min_samples` keywords.  Here are some good values to start with:\n",
    "\n",
    "```yaml\n",
    "reducer_config:\n",
    "    point_reducer_hdbscan:\n",
    "        min_cluster_size: 4\n",
    "        min_samples: 3\n",
    "```\n",
    "\n",
    "#### Converting the reducer config for use on Caesar\n",
    "\n",
    "All of the reducers available in within `panoptes_aggregation` can be used as \"external extractors\" on Caesar.\n",
    "\n",
    "1. On the \"Reducers\" tab in Caesar click \"Crate Reducer\" and select \"External\"\n",
    "2. Enter a unique \"key\" to reference this reducer later\n",
    "    - This value is used by the rules later and will show up in the data export\n",
    "3. Enter the \"URL\" as `https://aggregation-caesar.zooniverse.org/reducers/<reducer name>?<param 1>=<value 1>&<param 2>=<value 2>&<etc ...>`\n",
    "    - For this example \n",
    "    ```\n",
    "    https://aggregation-caesar.zooniverse.org/reducers/point_reducer_hdbscan?min_cluster_size=4&min_samples=3\n",
    "    ```\n",
    "4. Expand the \"Filters\" section\n",
    "5. Fill in the \"Extractor keys\" section as a list \n",
    "    - This the key picked in step 2 of the extractor config\n",
    "6. Pick how you want \"Repeated Classifications\" to be handled\n",
    "    - Defaults to \"keep first\"\n",
    "    - \"keep all\" can be useful at the testing/debugging stage\n",
    "7. Click \"Create External reducer\"\n",
    "\n",
    "Note: For question tasks you will typically want to use the built in stats reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the reducers\n",
    "See `panoptes_aggregation reduce -h` for help text explaining each of these inputs.\n",
    "\n",
    "Note: By default only if a volunteer classifies the same subject multiple times only the first one is used. This can be changed with the `-F` flag on the command line (e.g.  `-F all` to keep all, `-F first` to keep first, `-F last` to keep last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wnL-m82oyA8q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing: 100% |###############################################| Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "!panoptes_aggregation reduce aggregation_results/question_extractor_example.csv aggregation_results/Reducer_config_workflow_6465_V52.76_question_extractor.yaml -o example -d aggregation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iw991ZbxyA8r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing: 100% |###############################################| Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "!panoptes_aggregation reduce aggregation_results/shortcut_extractor_example.csv aggregation_results/Reducer_config_workflow_6465_V52.76_shortcut_extractor.yaml -o example -d aggregation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5mxjVTSeyA8r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing: 100% |###############################################| Time:  0:00:01\n"
     ]
    }
   ],
   "source": [
    "!panoptes_aggregation reduce aggregation_results/point_extractor_by_frame_example.csv aggregation_results/Reducer_config_workflow_6465_V52.76_point_extractor_by_frame.yaml -o example -d aggregation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD96TM4qyA8r"
   },
   "source": [
    "### Plot the results\n",
    "\n",
    "The final step is examining the results of the point clustering.  A `jupyter notebook` named `plotting_functions.ipynb` is included with the shared files, this can be run by either opening the notebook directly or running the command `jupyter lab` in the folder containing the file, and opening it in your web browser.  User `shift+enter` to run the code in each of the cells.\n",
    "\n",
    "Note: you will have to adjust the file paths in the \"Reading in the data\" and \"Plotting all the images\" sections to match your file set up.\n",
    "\n",
    "### Understanding what the extraction and reduction files contain\n",
    "\n",
    "The columns of the question extractor and question reducer files contain some data already described above and the counts for each of the possible answers for each question.\n",
    "\n",
    "#### Point extractor\n",
    "\n",
    "There are `x` and `y` data for each of the four point drawing tools:\n",
    "\n",
    "* `data.frame0.T0_tool*_x`: A list of the `x` data for each point created for `tool*`\n",
    "* `data.frame0.T0_tool*_y`: A list of the `y` data for each point created for `tool*`\n",
    "\n",
    "#### Point reducer\n",
    "\n",
    "In addition to the original point data:\n",
    "\n",
    "* `data.frame0.T0_tool*_points_x` : A list of `x` positions for **all** points drawn with `tool*`\n",
    "* `data.frame0.T0_tool*_points_y` : A list of `y` positions for **all** points drawn with `tool*`\n",
    "* `data.frame0.T0_tool*_cluster_labels` : A list of cluster labels for **all** points drawn with `tool*`\n",
    "* `data.frame0.T0_tool*_cluster_probabilities`: A list of cluster probabilities for **all** points drawn with `tool*`\n",
    "* `data.frame0.T0_tool*_clusters_persistance`: A measure for how persistent each **cluster** is (1.0 = stable, 0.0 = unstable)\n",
    "* `data.frame0.T0_tool*_clusters_count` : The number of points in each **cluster** found\n",
    "* `data.frame0.T0_tool*_clusters_x` : The weighted `x` position for each **cluster** found\n",
    "* `data.frame0.T0_tool*_clusters_y` : The weighted `y` position for each **cluster** found\n",
    "* `data.frame0.T0_tool*_clusters_var_x` : The weighted `x` variance of points in each **cluster** found\n",
    "* `data.frame0.T0_tool*_clusters_var_y` : The weighted `y` variance of points in each **cluster** found\n",
    "* `data.frame0.T0_tool*_clusters_var_x_y` : The weighted `x-y` covariance of points in each **cluster** found\n",
    "\n",
    "## Other things to try\n",
    "\n",
    "* Play around with changing the `min_cluster_size` and `min_samples` parameters to see how they change the detected clusters\n",
    "* Read the various `csv` files into you favorite programming language and explore the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J37wu1psyA8s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Working_with_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
